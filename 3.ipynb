{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://98f358221d01:4040\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1607796816619)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\n",
       "import org.apache.spark.mllib.evaluation.{BinaryClassificationMetrics, MulticlassMetrics}\n",
       "import org.apache.spark.mllib.linalg.Vectors\n",
       "import org.apache.spark.mllib.optimization.{HingeGradient, LBFGS, SquaredL2Updater}\n",
       "import org.apache.spark.mllib.util.MLUtils\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\n",
    "import org.apache.spark.mllib.evaluation.{BinaryClassificationMetrics, MulticlassMetrics}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.optimization.{HingeGradient, LBFGS, SquaredL2Updater}\n",
    "import org.apache.spark.mllib.util.MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@17841ec2\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sc = org.apache.spark.sql.SparkSession.builder()\n",
    "    .appName(\"KIS-DE-HW-3\")\n",
    "    .master(\"local\")\n",
    "    .getOrCreate()\n",
    "    .sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предопределим гиперпараметры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kIterations: Integer = 1024\n",
       "numCorrections: Integer = 20\n",
       "convergenceTol: Double = 1.0E-6\n",
       "regParam: Double = 0.1\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kIterations:    Integer = 1024    \n",
    "val numCorrections: Integer = 20\n",
    "val convergenceTol: Double  = 1e-6\n",
    "val regParam:       Double  = 0.1,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные. Сделаем разбиение на `train` и `test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainTestSplit: Array[Double] = Array(0.8, 0.2)\n",
       "randomSeed: Long = 42\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainTestSplit: Array[Double] = Array(0.8,0.2)\n",
    "val randomSeed:     Long          = 42L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:86\n",
       "numFeatures: Int = 185316\n",
       "splits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[7] at randomSplit at <console>:36, MapPartitionsRDD[8] at randomSplit at <console>:36)\n",
       "train: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at <console>:36\n",
       "test: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at randomSplit at <console>:36\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataset     = MLUtils.loadLibSVMFile(sc, \"dataset.libsvm\")\n",
    "val numFeatures = dataset.take(1)(0).features.size\n",
    "val splits      = dataset.randomSplit(trainTestSplit, seed = randomSeed)\n",
    "val train       = splits(0)\n",
    "val test        = splits(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelSVMWithSGD: org.apache.spark.mllib.classification.SVMModel = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 185316, numClasses = 2, threshold = 0.0\n",
       "scoreAndLabelsSVMWithSGD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[300] at map at <console>:35\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val modelSVMWithSGD          = SVMWithSGD.train(train, kIterations)\n",
    "val scoreAndLabelsSVMWithSGD = test.map { point =>\n",
    "  val score = modelSVMWithSGD.predict(point.features)\n",
    "  (score, point.label)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weightsWithIntercept: org.apache.spark.mllib.linalg.Vector = [2.822551961091748E-10,0.0,2.822551961091748E-10,8.467655883274513E-10,8.467655883274513E-10,4.2338279416372566E-10,-8.330058803622971E-5,6.7741247066196105E-9,1.411275980545874E-10,8.467655883274513E-10,1.411275980545874E-10,1.5524035786003775E-9,7.056379902729038E-10,8.368481087798259E-5,2.822551961091748E-10,1.5524035786003775E-9,1.411275980545874E-10,1.411275980545874E-10,8.467655883274513E-10,1.411275980545874E-10,2.822551961091748E-10,9.87893186382057E-10,1.411275980545874E-10,1.411275980545874E-10,-8.367577871170714E-5,1.411275980545874E-10,1.411275980545874E-10,1.411275980545874E-10,1.411275980545874E-10,4.2338279416372566E-10,1.411275980545874E-10,0.0,-5.443430465663548E-7,1.411275980545874E-10,-8.36776133704819E-5,3....\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (weightsWithIntercept, _) = LBFGS.runLBFGS(\n",
    "    train.map{ x => \n",
    "        (x.label, MLUtils.appendBias(x.features))\n",
    "    },\n",
    "    new HingeGradient,\n",
    "    new SquaredL2Updater,\n",
    "    numCorrections   = numCorrections,\n",
    "    convergenceTol   = convergenceTol,\n",
    "    maxNumIterations = kIterations,\n",
    "    regParam         = regParam,\n",
    "    initialWeights   = Vectors.dense(new Array[Double](numFeatures + 1))\n",
    ")\n",
    "\n",
    "val modelSVMWithLBFGS = new SVMModel(\n",
    "    Vectors.dense(weightsWithIntercept.toArray.init),\n",
    "    weightsWithIntercept.toArray.last\n",
    ")\n",
    "\n",
    "val scoreAndLabelsSVMWithLBFGS = test.map { point =>\n",
    "  val score = modelSVMWithLBFGS.predict(point.features)\n",
    "  (score, point.label)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROCWithSGD   = 0.5\n",
      "areaUnderROCWithLBFGS = 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "metricsSVMWithSGD: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@5d91b736\n",
       "areaUnderROCWithSGD: Double = 0.5\n",
       "metricsSVMWithLBFGS: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@c4e3954\n",
       "areaUnderROCWithLBFGS: Double = 0.5\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metricsSVMWithSGD     = new BinaryClassificationMetrics(scoreAndLabelsSVMWithSGD)\n",
    "val areaUnderROCWithSGD   = metricsSVMWithSGD.areaUnderROC\n",
    "val metricsSVMWithLBFGS   = new BinaryClassificationMetrics(scoreAndLabelsSVMWithLBFGS)\n",
    "val areaUnderROCWithLBFGS = metricsSVMWithLBFGS.areaUnderROC\n",
    "\n",
    "println(s\"\"\"areaUnderROCWithSGD   = ${areaUnderROCWithSGD}\n",
    "           |areaUnderROCWithLBFGS = ${areaUnderROCWithLBFGS}\"\"\".stripMargin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracyWithSGD   = 0.9993303421951383\n",
      "accuracyWithLBFGS = 0.9993303421951383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "metricSVMWithSGD: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@257265ad\n",
       "metricSVMWithLBFGS: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@4f210128\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metricSVMWithSGD   = new MulticlassMetrics(scoreAndLabelsSVMWithSGD)\n",
    "val metricSVMWithLBFGS = new MulticlassMetrics(scoreAndLabelsSVMWithLBFGS)\n",
    "\n",
    "println(s\"\"\"accuracyWithSGD   = ${metricSVMWithSGD.accuracy}\n",
    "           |accuracyWithLBFGS = ${metricSVMWithLBFGS.accuracy}\"\"\".stripMargin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод:  \n",
    "AUC и accuracy вышли одинаковы. ML-ресерчер с меня никакой:)  \n",
    "Есть единственное предположение - это значительное преобладание одного класса над другим, давайте проверим это:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: scala.collection.Map[Double,Long] = Map(0.0 -> 149211, 1.0 -> 178)\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.map(_.label).countByValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет фигня, будем предсказывать только нули. Весь код приложенный выше чисто для демонстрации, что я могу простой ML на Spark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
