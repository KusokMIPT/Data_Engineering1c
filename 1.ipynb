{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Схема данных\n",
    "\n",
    "| customer               |                                                                                                                                                       |\n",
    "|------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| customer_id            | Идентификатор клиента                                                                                                                                 |\n",
    "| product_X              | Статус продукта. OPN - открыт, но не утилизирован. UTL - утилизирован. CLS - закрыт                                                                   |\n",
    "| gender_cd              | Пол. M - мужской. F - женский                                                                                                                         |\n",
    "| age                    | Возраст в годах                                                                                                                                       |\n",
    "| marital_status_cd      | Семейный статус. См. словарь соответствия                                                                                                             |\n",
    "| children_cnt           | Количество детей в штуках                                                                                                                             |\n",
    "| first_session_dttm     | Дата и время первой сессии в приложении или личном кабинете на сайте                                                                                  |\n",
    "| job_position_cd        | Категория занимаемой должности. См. словарь соответствия                                                                                              |\n",
    "| job_title              | Занимаемая должность                                                                                                                                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| stories_reaction_train |                                                                                                                                                       |\n",
    "|------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| customer_id            | Идентификатор клиента                                                                                                                                 |\n",
    "| story_id               | Идентификатор истории                                                                                                                                 |\n",
    "| event_dttm             | Дата действия                                                                                                                                         |\n",
    "| event                  | Тип действия. like - лайк или сохранение. view - глубокий просмотр (более 10 секунд). skip - пролистанная история (менее 5 секунд). dislike - дизлайк |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd._\n",
       "defined class SimpleCSVHeader\n",
       "csvCustomer: org.apache.spark.rdd.RDD[String] = ./customer_train.csv MapPartitionsRDD[1] at textFile at <console>:32\n",
       "dataCustomer: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[2] at map at <console>:33\n",
       "headerCustomer: SimpleCSVHeader = SimpleCSVHeader@7dab0178\n",
       "customers: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[3] at filter at <console>:35\n",
       "csvStories: org.apache.spark.rdd.RDD[String] = ./stories_reaction_train.csv MapPartitionsRDD[5] at textFile at <console>:37\n",
       "dataStories: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[6] at map at <console>:38\n",
       "headerStories: SimpleCSVHeader = SimpleCSVHeader@796b3b1\n",
       "stories: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[7] at filter at ...\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd._\n",
    "\n",
    "class SimpleCSVHeader(header:Array[String]) extends Serializable {\n",
    "   \n",
    "  val index = header.zipWithIndex.toMap\n",
    "  def apply(array:Array[String], key:String) : String = {\n",
    "      val curIndex = index(key)\n",
    "      if (curIndex < array.size) {\n",
    "          return array(curIndex)\n",
    "      } else {\n",
    "          return \"\"\n",
    "      }\n",
    "  }\n",
    "}\n",
    "\n",
    "val csvCustomer = sc.textFile(\"./customer_train.csv\")  // original file\n",
    "val dataCustomer = csvCustomer.map(line => line.split(\",\").map(elem => elem.trim)) //lines in rows\n",
    "val headerCustomer = new SimpleCSVHeader(dataCustomer.first()) // we build our header with the first line\n",
    "val customers = dataCustomer.filter(line => headerCustomer(line, \"customer_id\") != \"customer_id\") // filter the header out\n",
    "\n",
    "val csvStories = sc.textFile(\"./stories_reaction_train.csv\")  // original file\n",
    "val dataStories = csvStories.map(line => line.split(\",\").map(elem => elem.trim)) //lines in rows\n",
    "val headerStories = new SimpleCSVHeader(dataStories.first()) // we build our header with the first line\n",
    "val stories = dataStories.filter(line => headerStories(line,\"customer_id\") != \"customer_id\") // filter the header out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[Array[String]] = Array(Array(customer_id, product_0, product_1, product_2, product_3, product_4, product_5, product_6, gender_cd, age, marital_status_cd, children_cnt, first_session_dttm, job_position_cd, job_title), Array(894436, \"\", \"\", \"\", \"\", \"\", UTL, \"\", M, 30.0, MAR, 0.0, 2018-03-20 09:10:16, 1, Неруководящий сотрудник - обсл. Персонал), Array(524526, \"\", UTL, \"\", \"\", \"\", UTL, \"\", F, 20.0, UNM, 0.0, 2017-03-29 20:38:45, 16), Array(498134, \"\", UTL, \"\", \"\", \"\", \"\", \"\", F, 25.0, UNM, 0.0, 2018-03-12 11:25:06, 22), Array(278941, \"\", \"\", UTL, CLS, \"\", UTL, UTL, M, 25.0, \"\", \"\", 2016-02-21 18:47:51, 16, Неруководящий сотрудник - специалист), Array(877312, \"\", UTL, \"\", \"\", \"\", \"\", \"\", F, 40.0, MAR, 0.0, 2018-03-07 11:17:02, 22), Array(821806, \"\", \"\", \"\", UTL, \"\", UTL, \"\", F, ...\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataCustomer.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Посчитать количество пользователей без детей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cnt_child_free_person: Int = 37284\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cnt_child_free_person = customers\n",
    "    .map(s => if (headerCustomer(s, \"children_cnt\") != \"\" && headerCustomer(s, \"children_cnt\").toDouble < 1) 1 else 0)\n",
    "    .reduce((a,b) => (a + b)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Посчитать долю пользователей старше 40 лет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "old_person_cnt: (Int, Int) = (8425,50000)\n",
       "res18: Double = 0.1685\n"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var old_person_cnt = customers\n",
    "    .map(s => if (headerCustomer(s, \"age\") != \"\" && headerCustomer(s, \"age\").toDouble > 40) 1 else 0)\n",
    "    .aggregate((0,0))(\n",
    "        (u,t) => (u._1 + t, u._2 + 1),\n",
    "        (u1,u2) => (u1._1 + u2._1, u1._2 + u2._2)\n",
    "    )\n",
    "\n",
    "old_person_cnt._1.toDouble / old_person_cnt._2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Посчитать количество историй, которые лайкнули люди, утилизировавшие продукт 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "utilization_2_prodRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[9] at map at <console>:35\n",
       "res2: Long = 531\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val utilization_2_prodRDD = customers\n",
    "    .filter(s => (headerCustomer(s, \"product_2\") == \"UTL\"))\n",
    "    .map(s => (headerCustomer(s, \"customer_id\"), 1))\n",
    "\n",
    "stories\n",
    "    .filter(s => (headerStories(s, \"event\") == \"like\"))\n",
    "    .map(s => (headerStories(s, \"customer_id\"), headerStories(s, \"story_id\")))\n",
    "    .join(utilization_2_prodRDD)\n",
    "    .map(s => s._2._1)\n",
    "    .distinct() // оставляем только уникальные итсории (а они могу повторяться, если у них будет разноее время например)\n",
    "    .count\n",
    "\n",
    "// customer_id story_id 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. С помощью flatMap посчитать у каждого пользователя количество продуктов по статусу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res37: Array[(String, (Int, Int, Int))] = Array((894436,(0,1,0)), (524526,(0,2,0)), (498134,(0,1,0)), (278941,(0,3,1)), (877312,(0,1,0)), (821806,(0,2,0)), (782728,(0,1,0)), (540071,(0,1,0)), (66158,(0,2,0)), (804202,(0,2,0)), (323753,(0,2,0)), (244719,(0,2,0)), (110927,(1,2,0)), (307008,(0,1,2)), (643021,(0,1,0)), (739814,(0,2,0)), (816597,(0,1,0)), (387428,(1,1,0)), (134847,(0,2,0)), (650669,(0,2,0)), (210331,(0,2,0)), (143055,(0,2,0)), (106750,(0,2,0)), (392634,(0,2,0)), (195762,(0,1,1)), (902650,(0,1,0)), (761975,(0,1,0)), (505245,(0,1,0)), (480749,(0,1,0)), (44824,(0,1,0)), (11028,(0,2,0)), (239164,(0,1,0)), (830854,(0,1,1)), (303498,(0,2,0)), (478642,(0,4,1)), (442584,(0,1,0)), (341657,(0,2,0)), (663082,(0,2,0)), (623830,(0,1,0)), (802333,(0,1,0)), (598106,(0,0,0)), (762259,(0,1,0...\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers\n",
    "    .map(s => (headerCustomer(s, \"customer_id\"),\n",
    "               s\n",
    "               .aggregate((0, 0, 0))(\n",
    "                   (u, t) => (u._1 + (if (t == \"OPN\") 1 else 0),\n",
    "                              u._2 + (if (t == \"UTL\") 1 else 0),\n",
    "                              u._3 + (if (t == \"CLS\") 1 else 0)),\n",
    "                   (u1, u2) => (u1._1 + u2._2, u1._2 + u2._2, u1._3 + u2._3)\n",
    "                   )\n",
    "               )\n",
    "        )//.count\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_в чате было написнао что flatMap можно не использовать )_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее во всех задачах для функций max\\min будем использовать: \n",
    "```\n",
    "rdd.max()(new Ordering[Tuple2[String, Int]]() {\n",
    "  override def compare(x: (String, Int), y: (String, Int)): Int = \n",
    "      Ordering[Int].compare(x._2, y._2)\n",
    "})\n",
    "```\n",
    "\n",
    "найденно тут: _https://stackoverflow.com/questions/26886275/how-to-find-max-value-in-pair-rdd_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Определить даты, в которые была наибольшая и наименьшая доля лайков историй от мужчин"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "men_stories: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[27] at map at <console>:34\n",
       "freq_mens_likes: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[35] at map at <console>:42\n",
       "ans_min: String = 2018-06-29\n",
       "ans_max: String = 2018-07-29\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val men_stories = customers\n",
    "    .map(s => (headerCustomer(s, \"customer_id\"), if (headerCustomer(s, \"gender_cd\") == \"M\") 1 else 0))\n",
    "\n",
    "val freq_mens_likes = stories\n",
    "    .filter(s => headerStories(s, \"event\") == \"like\")\n",
    "    .map(s => (headerStories(s, \"customer_id\"), headerStories(s, \"event_dttm\")))\n",
    "    .join(men_stories)\n",
    "    .map(s => (s._2._1.slice(0, 10), (s._2._2, 1))) //(data, 1 if like, 1)\n",
    "    .reduceByKey((v1, v2) => (v1._1 + v2._1, v1._2 + v2._2)) // calc count \n",
    "    .map(s => (s._1, s._2._1.toDouble / s._2._2))\n",
    "\n",
    "val ans_min = freq_mens_likes.max()(new Ordering[Tuple2[String, Double]]() {\n",
    "  override def compare(x: (String, Double), y: (String, Double)): Int = \n",
    "      Ordering[Double].compare(x._2, y._2)\n",
    "})._1\n",
    "\n",
    "val ans_max = freq_mens_likes.min()(new Ordering[Tuple2[String, Double]]() {\n",
    "  override def compare(x: (String, Double), y: (String, Double)): Int = \n",
    "      Ordering[Double].compare(x._2, y._2)\n",
    "})._1                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Среди тех, кто посмотрел историю 138, найдите id пользователя с максимальным количеством детей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "viewers138: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[377] at map at <console>:36\n",
       "res60: (String, Double) = (143885,3.0)\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val viewers138 = stories\n",
    "    .filter(s => headerStories(s, \"event\") == \"view\" && headerStories(s, \"story_id\") == \"138\")\n",
    "    .map(s => (headerStories(s, \"customer_id\"), 1))\n",
    "\n",
    "customers\n",
    "    .filter(s => headerCustomer(s, \"children_cnt\")!= \"\")\n",
    "    .map(s => (headerCustomer(s, \"customer_id\"), headerCustomer(s, \"children_cnt\").toDouble))\n",
    "    .join(viewers138) // (customer_id, (children_cnt, 1))\n",
    "    .map(s => (s._1, s._2._1))\n",
    "    .max()(new Ordering[Tuple2[String, Double]]() {\n",
    "      override def compare(x: (String, Double), y: (String, Double)): Int = \n",
    "          Ordering[Double].compare(x._2, y._2)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Найдите id истории с наибольшим отношением skip'ов к like'ам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: String = 171\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories\n",
    "    .map(a => (headerStories(a, \"story_id\"), \n",
    "               if (headerStories(a, \"event\") == \"like\") (1.0, 0) else\n",
    "               if (headerStories(a, \"event\") == \"skip\") (0.0, 1) \n",
    "               else (0.0, 0)\n",
    "              )\n",
    "        )\n",
    "    .reduceByKey((v1, v2) => (v1._1 + v2._1, v1._2 + v2._2))\n",
    "    .filter(s => s._2._1 > 0) // at least one like\n",
    "    .map(s => (s._1, s._2._2 / s._2._1))\n",
    "    .max()(new Ordering[Tuple2[String, Double]]() {\n",
    "      override def compare(x: (String, Double), y: (String, Double)): Int = \n",
    "          Ordering[Double].compare(x._2, y._2)\n",
    "    })._1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Напишите tail recursive функцию конкатенации листа листов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Доработаем код с семинара, по аналогии с вычислением факториала, за основу возьмем flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flatten: (list: List[Any])List[Any]\n",
       "nested: List[Any] = List(1, List(2, 3), 4)\n",
       "flat: List[Any] = List(1, 2, 3, 4)\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten(list: List[Any]): List[Any]=\n",
    "    list match{\n",
    "        case (x : List[Any]) :: xs => flatten(x) ::: flatten(xs)\n",
    "        case x :: xs => x :: flatten (xs)\n",
    "        case Nil => Nil\n",
    "    }\n",
    "\n",
    "val nested = List(1, List(2,3), 4)\n",
    "val flat = flatten(nested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.annotation.tailrec\n",
       "flatten: (arr: List[List[Any]], built: List[Any])List[Any]\n",
       "nested: List[List[Int]] = List(List(2, 3))\n",
       "flat: List[Any] = List(2, 3)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.annotation.tailrec\n",
    "\n",
    "@tailrec\n",
    "def flatten(arr: List[List[Any]], built: List[Any] = Nil): List[Any] = \n",
    "    arr match {\n",
    "        case l :: (x: List[List[Any]]) => flatten(x, built ::: l)\n",
    "        case l1 :: l2 => flatten(Nil, built ::: l1 ::: l2)\n",
    "        case Nil => built\n",
    "    }\n",
    "\n",
    "val nested = List(List(2,3))\n",
    "val flat = flatten(nested)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S понятно что у нас упрощенная задача, и на таком примере: `((1,2),3, 4)` она не обязана работать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
